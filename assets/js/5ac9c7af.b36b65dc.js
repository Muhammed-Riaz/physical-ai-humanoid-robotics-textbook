"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[386],{8325:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>l,toc:()=>c});var o=i(4848),t=i(8453);const s={title:"Module 4 - Vision-Language-Action (VLA)",sidebar_position:5},a="Module 4: Vision-Language-Action (VLA) - Week 13",l={id:"modules/vla/index",title:"Module 4 - Vision-Language-Action (VLA)",description:"Overview",source:"@site/docs/modules/04-vla/index.md",sourceDirName:"modules/04-vla",slug:"/modules/vla/",permalink:"/physical-ai-humanoid-robotics-textbook/modules/vla/",draft:!1,unlisted:!1,editUrl:"https://github.com/Muhammed-Riaz/physical-ai-humanoid-robotics-textbook/edit/main/docs/docs/modules/04-vla/index.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{title:"Module 4 - Vision-Language-Action (VLA)",sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Nav2 Deployment on Jetson Orin",permalink:"/physical-ai-humanoid-robotics-textbook/modules/isaac/nav2_jetson"},next:{title:"Weeks 1-2 - Introduction to Physical AI",permalink:"/physical-ai-humanoid-robotics-textbook/weeks/week-01_02/"}},r={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Key Concepts with Analogies",id:"key-concepts-with-analogies",level:2},{value:"Hands-on Lab: Creating a Voice-Controlled Robot",id:"hands-on-lab-creating-a-voice-controlled-robot",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Steps",id:"steps",level:3},{value:"Quiz/Questions",id:"quizquestions",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla---week-13",children:"Module 4: Vision-Language-Action (VLA) - Week 13"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Vision-Language-Action (VLA) represents the convergence of LLMs and Robotics, enabling robots to understand natural language commands and execute them. This module covers voice-to-action systems using OpenAI Whisper and cognitive planning to translate natural language into ROS 2 actions."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this module, students will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement voice-to-action systems using OpenAI Whisper"}),"\n",(0,o.jsx)(n.li,{children:"Use LLMs for cognitive planning in robotics"}),"\n",(0,o.jsx)(n.li,{children:'Translate natural language ("Clean the room") into sequences of ROS 2 actions'}),"\n",(0,o.jsx)(n.li,{children:"Integrate multimodal perception with language understanding"}),"\n",(0,o.jsx)(n.li,{children:"Build conversational interfaces for robots"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"key-concepts-with-analogies",children:"Key Concepts with Analogies"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"VLA Pipeline"}),": Like a human who hears a command, understands it, and acts upon it, VLA enables robots to process language and execute actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Cognitive Planning"}),": Like how humans mentally plan a sequence of steps to complete a task, cognitive planning translates high-level commands into executable actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multimodal Integration"}),": Like how humans combine vision, hearing, and action, multimodal systems integrate different sensory inputs with motor outputs"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"hands-on-lab-creating-a-voice-controlled-robot",children:"Hands-on Lab: Creating a Voice-Controlled Robot"}),"\n",(0,o.jsx)(n.p,{children:"In this lab, students will create a robot that responds to voice commands using VLA techniques."}),"\n",(0,o.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Ubuntu 22.04"}),"\n",(0,o.jsx)(n.li,{children:"Microphone for voice input"}),"\n",(0,o.jsx)(n.li,{children:"Understanding of ROS 2 and basic LLM integration"}),"\n",(0,o.jsx)(n.li,{children:"Completed previous modules"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Set up voice recognition using OpenAI Whisper"}),"\n",(0,o.jsx)(n.li,{children:"Integrate with an LLM for cognitive planning"}),"\n",(0,o.jsx)(n.li,{children:"Map natural language to ROS 2 actions"}),"\n",(0,o.jsx)(n.li,{children:"Test with simulated humanoid robot"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate response accuracy and execution"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"quizquestions",children:"Quiz/Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"What are the challenges in implementing voice-to-action systems for robots?"}),"\n",(0,o.jsx)(n.li,{children:"How does cognitive planning differ from simple command mapping?"}),"\n",(0,o.jsx)(n.li,{children:"What are the benefits of multimodal integration in robotics?"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://openai.com/research/whisper",children:"OpenAI Whisper Documentation"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2209.06167",children:"Vision-Language-Action Models"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://ieeexplore.ieee.org/document/8983084",children:"Conversational Robotics"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var o=i(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[94],{1148:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var s=i(4848),t=i(8453);const o={title:"Capstone Project - Step-by-Step Guide",sidebar_position:14},a="Capstone Project: Step-by-Step Guide",r={id:"capstone/tasks",title:"Capstone Project - Step-by-Step Guide",description:"Overview",source:"@site/docs/capstone/tasks.md",sourceDirName:"capstone",slug:"/capstone/tasks",permalink:"/physical-ai-humanoid-robotics-textbook/capstone/tasks",draft:!1,unlisted:!1,editUrl:"https://github.com/Muhammed-Riaz/physical-ai-humanoid-robotics-textbook/edit/main/docs/docs/capstone/tasks.md",tags:[],version:"current",sidebarPosition:14,frontMatter:{title:"Capstone Project - Step-by-Step Guide",sidebar_position:14},sidebar:"tutorialSidebar",previous:{title:"Capstone Project - The Autonomous Humanoid",permalink:"/physical-ai-humanoid-robotics-textbook/capstone/"},next:{title:"Hardware Guide - Building Your Physical AI Lab",permalink:"/physical-ai-humanoid-robotics-textbook/hardware-guide/"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Phase 1: Environment and Robot Model Setup",id:"phase-1-environment-and-robot-model-setup",level:2},{value:"Phase 2: Perception Integration",id:"phase-2-perception-integration",level:2},{value:"Phase 3: Navigation Stack Implementation",id:"phase-3-navigation-stack-implementation",level:2},{value:"Phase 4: Vision-Language-Action (VLA) Pipeline",id:"phase-4-vision-language-action-vla-pipeline",level:2},{value:"Phase 5: Autonomous Execution and Validation",id:"phase-5-autonomous-execution-and-validation",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"capstone-project-step-by-step-guide",children:"Capstone Project: Step-by-Step Guide"}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This guide provides a detailed step-by-step approach to implementing the Autonomous Humanoid Capstone Project. Follow these instructions to integrate all components and achieve autonomous operation."}),"\n",(0,s.jsx)(n.h2,{id:"phase-1-environment-and-robot-model-setup",children:"Phase 1: Environment and Robot Model Setup"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Set up a new simulation environment in Gazebo or Isaac Sim."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Create a simple indoor environment with obstacles (tables, chairs, walls)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Launch the simulation and ensure the environment loads correctly."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Import or create a humanoid robot model."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Use a URDF model of a humanoid robot and ensure it has all necessary joints and links. Integrate the model into your chosen simulator."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Spawn the robot in the simulation and confirm it appears correctly with articulated joints."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Configure ROS 2 bridge for robot control."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Set up the necessary ROS 2 nodes and topics for controlling the robot's joints and receiving sensor data from the simulator."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Publish simple joint commands and observe robot movement in simulation; subscribe to sensor topics and verify data flow."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"phase-2-perception-integration",children:"Phase 2: Perception Integration"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Integrate simulated camera and LiDAR sensors."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Add camera and LiDAR plugins to your robot model in the simulator. Configure their topics and frame IDs."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Visualize camera images and LiDAR scans in RViz; ensure data is being published on correct ROS 2 topics."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Implement VSLAM for localization and mapping."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Deploy the VSLAM pipeline (e.g., Isaac ROS Visual SLAM) on a simulated Jetson Orin. Configure it to use data from your simulated sensors."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Build a map of the environment and track the robot's pose accurately in RViz or a mapping visualization tool."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Develop object recognition capabilities."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),': Integrate an object detection model (e.g., YOLO, trained on synthetic data from Isaac Sim) to identify specific objects in the simulated environment (e.g., "red cup", "blue box").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Run object detection on simulated camera feeds and confirm correct object identification and bounding box detection."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"phase-3-navigation-stack-implementation",children:"Phase 3: Navigation Stack Implementation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Set up Nav2 for path planning and obstacle avoidance."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Configure Nav2 parameters, including global and local costmaps, planner, and controller. Ensure it uses the VSLAM map for global localization."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Launch Nav2 and visualize costmaps, global plans, and local plans in RViz. Send a navigation goal and observe the robot planning a path."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Implement bipedal humanoid movement control."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Develop a ROS 2 controller that translates Nav2's linear and angular velocity commands into stable bipedal locomotion commands for your humanoid robot."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Command the robot to move to a simple goal and evaluate its stability and ability to reach the target without falling or excessive wobbling."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"phase-4-vision-language-action-vla-pipeline",children:"Phase 4: Vision-Language-Action (VLA) Pipeline"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Integrate speech recognition (OpenAI Whisper)."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Set up a microphone (simulated or real) and use OpenAI Whisper to convert spoken commands into text."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Speak commands and confirm accurate transcription into text messages via a ROS 2 topic or console output."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Develop cognitive planning with an LLM."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),': Use a large language model (LLM) to translate high-level natural language commands (e.g., "pick up the red cup") into a sequence of low-level ROS 2 actions (e.g., move_to, grasp, speak).']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Input various natural language commands and observe the LLM generating logical action sequences. Ensure robust error handling for unknown commands."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Implement action execution module."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Create a ROS 2 node that interprets the LLM's action sequence and triggers corresponding robot behaviors (navigation goals, grasping commands, speech output)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Test individual actions (move, grasp, speak) and ensure they are executed correctly by the robot."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"phase-5-autonomous-execution-and-validation",children:"Phase 5: Autonomous Execution and Validation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Combine all VLA, navigation, and perception modules."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Create a top-level ROS 2 launch file that starts all necessary nodes (sensors, VSLAM, Nav2, VLA pipeline) in the correct order with appropriate dependencies."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Launch the full system and ensure all components initialize without errors."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Conduct full capstone scenario test."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),': Give the robot a voice command for a complex task (e.g., "Go to the kitchen, pick up the red cup, and bring it to me"). Observe and record its performance.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Evaluate whether the robot successfully navigates, recognizes the object, manipulates it, and returns, demonstrating full autonomy."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task"}),": Debug and optimize the complete system."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Details"}),": Identify and resolve any issues related to performance, accuracy, or robustness. Optimize parameters for smoother operation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verification"}),": Iterate on testing and optimization until the robot reliably completes the capstone task with high success rates."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"Congratulations on completing the capstone project! You have successfully built an autonomous humanoid robot. Consider refining your implementation, exploring additional features, or deploying your solution to real hardware."})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var s=i(6540);const t={},o=s.createContext(t);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);